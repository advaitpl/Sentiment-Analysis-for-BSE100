import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer, WordNetLemmatizer
import nltk
import re
import joblib

# Download stopwords and lemmatizer
nltk.download('stopwords')
nltk.download('wordnet')

# Step 1: Load the Data
data_path = 'data 2.csv'  # Update this path to your data file
df = pd.read_csv(data_path)

# Inspect the data
print(df.head())

# Step 2: Preprocess the Data
# Handling missing values (if any)
df.dropna(inplace=True)

# Advanced text preprocessing
stop_words = set(stopwords.words('english'))
stemmer = SnowballStemmer("english")
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'\b\w{1,2}\b', '', text)  # Remove words with 1 or 2 characters
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    text = ''.join([char for char in text if char.isalnum() or char.isspace()])
    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])
    return text

df['cleaned_text'] = df['Sentence'].apply(preprocess_text)  # Correct column name

# Encode labels
label_encoder = LabelEncoder()
df['encoded_sentiment'] = label_encoder.fit_transform(df['Sentiment'])

# Verify encoded labels
print("Encoded Sentiment Classes:", np.unique(df['encoded_sentiment']))

# Step 3: Feature Extraction
vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 3), analyzer='char_wb')
X = vectorizer.fit_transform(df['cleaned_text'])
y = df['encoded_sentiment']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Define Models
models = {
    "Naive Bayes": MultinomialNB(),
    "SVM": SVC(probability=True, random_state=42),
    "Logistic Regression": LogisticRegression(max_iter=500, random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42)
}

# Hyperparameters for GridSearchCV
params = {
    "Naive Bayes": {},
    "SVM": {
        'C': [0.1, 1, 10, 100],
        'kernel': ['linear', 'rbf']
    },
    "Logistic Regression": {
        'C': [0.1, 1, 10, 100]
    },
    "Random Forest": {
        'n_estimators': [100, 200, 300],
        'max_depth': [10, 20, None]
    },
    "Gradient Boosting": {
        'n_estimators': [100, 200, 300],
        'learning_rate': [0.01, 0.1, 0.3],
        'max_depth': [3, 5, 7]
    }
}

# Step 5: Train and Evaluate Models
results = {}
best_models = {}
for model_name, model in models.items():
    print(f"Training {model_name}...")
    clf = GridSearchCV(model, params[model_name], cv=5, n_jobs=-1, verbose=1)
    clf.fit(X_train, y_train)
    best_model = clf.best_estimator_
    best_models[model_name] = best_model
    y_pred = best_model.predict(X_test)
    
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    
    results[model_name] = {
        "best_params": clf.best_params_,
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1": f1
    }

# Ensemble Model
voting_clf = VotingClassifier(estimators=[
    ('nb', best_models['Naive Bayes']),
    ('svm', best_models['SVM']),
    ('lr', best_models['Logistic Regression']),
    ('rf', best_models['Random Forest']),
    ('gb', best_models['Gradient Boosting'])
], voting='soft')

voting_clf.fit(X_train, y_train)
y_pred = voting_clf.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

results["Voting Classifier"] = {
    "accuracy": accuracy,
    "precision": precision,
    "recall": recall,
    "f1": f1
}

# Print Results
for model_name, metrics in results.items():
    print(f"Results for {model_name}:")
    print(f"Best Params: {metrics.get('best_params', 'N/A')}")
    print(f"Accuracy: {metrics['accuracy']:.4f}")
    print(f"Precision: {metrics['precision']:.4f}")
    print(f"Recall: {metrics['recall']:.4f}")
    print(f'F1 Score: {metrics["f1"]:.4f}')
    print("\n")

# Optional: Save Models
for model_name, model in best_models.items():
    joblib.dump(model, f'{model_name}_model.pkl')

joblib.dump(voting_clf, 'Voting_Classifier_model.pkl')

# Save the vectorizer and label encoder
joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')
joblib.dump(label_encoder, 'label_encoder.pkl')
