import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.pipeline import Pipeline
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
import nltk
import joblib
import logging

# Initialize logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Download stopwords
nltk.download('stopwords')

# Step 1: Load the Data
data_path = '/Users/shubhishyamsukha/Downloads/data.csv'  # Update this path to your data file
try:
    df = pd.read_csv(data_path)
    logging.info("Data loaded successfully.")
except Exception as e:
    logging.error(f"Error loading data: {e}")
    raise

# Inspect the data
print(df.head())

# Step 2: Preprocess the Data
# Handling missing values (if any)
df.dropna(inplace=True)

# Advanced text preprocessing
stop_words = set(stopwords.words('english'))
stemmer = SnowballStemmer("english")

def preprocess_text(text):
    text = text.lower()
    text = ''.join([char for char in text if char.isalnum() or char.isspace()])
    text = ' '.join([stemmer.stem(word) for word in text.split() if word not in stop_words])
    return text

df['cleaned_text'] = df['Sentence'].apply(preprocess_text)  # Correct column name

# Step 3: Feature Extraction and Model Training with Pipeline
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),  # Including n-grams
    ('clf', RandomForestClassifier(random_state=42))
])

# Hyperparameter tuning
param_grid = {
    'clf__n_estimators': [100, 200],
    'clf__max_depth': [None, 10, 20],
    'clf__min_samples_split': [2, 5],
    'clf__min_samples_leaf': [1, 2]
}

grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)

# Split the data
X = df['cleaned_text']
y = df['Sentiment']  # Correct column name
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model with hyperparameter tuning
try:
    grid_search.fit(X_train, y_train)
    logging.info("Model training completed.")
except Exception as e:
    logging.error(f"Error during model training: {e}")
    raise

# Best model
best_model = grid_search.best_estimator_

# Step 5: Evaluate the Model
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')
conf_matrix = confusion_matrix(y_test, y_pred)

print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')
print(f'Confusion Matrix:\n{conf_matrix}')

# Save the Model
try:
    joblib.dump(best_model, 'financial_sentiment_model.pkl')
    joblib.dump(grid_search.best_params_, 'best_params.pkl')
    logging.info("Model and parameters saved successfully.")
except Exception as e:
    logging.error(f"Error saving model or parameters: {e}")
    raise
